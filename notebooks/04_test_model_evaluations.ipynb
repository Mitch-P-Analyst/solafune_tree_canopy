{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7502fe",
   "metadata": {},
   "source": [
    "# Test Model Evaluations\n",
    "\n",
    "## **Disclaimer**\n",
    "Optional Unfinished Notebook file. Containing indepth measures to analyse test split data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821c5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAths & Directories\n",
    "NB_DIR = Path.cwd()\n",
    "REPO_ROOT = NB_DIR.parent\n",
    "\n",
    "test_dir_path =  REPO_ROOT / 'data/processed/images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f8767d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote /Users/mitchellpalmer/Projects/solafune-canopy-capstone-clean/data/processed/JSONs/test_annotations_coco.json\n",
      "   23 images, 6569 annotations, 2 categories\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- paths & directories---\n",
    "REPO_ROOT = Path.cwd().parent\n",
    "COCO_IN   = REPO_ROOT / 'data/processed/JSONs/train_annotations_coco.json'   # source (full) COCO\n",
    "COCO_OUT  = REPO_ROOT / 'data/processed/JSONs/test_annotations_coco.json'    # where to write subset\n",
    "\n",
    "# Gather Test Images\n",
    "test_names = {p.name for p in (REPO_ROOT/'data/processed/images/test').glob('*.tif')}\n",
    "\n",
    "# --- load full COCO ---\n",
    "with open(COCO_IN, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Index images by file_name for O(1) lookups\n",
    "name_to_img = {img['file_name']: img for img in coco['images']}\n",
    "\n",
    "# Warn about any filenames in the test split that aren’t in COCO\n",
    "missing = sorted(test_names - set(name_to_img.keys()))\n",
    "if missing:\n",
    "    print(f\"⚠️ {len(missing)} test images not found in COCO JSON (showing up to 5): {missing[:5]}\")\n",
    "\n",
    "# Keep only images present in COCO\n",
    "subset_images = [name_to_img[n] for n in test_names if n in name_to_img]\n",
    "subset_image_ids = {img['id'] for img in subset_images}\n",
    "\n",
    "# Keep only annotations that belong to those images\n",
    "subset_anns = [ann for ann in coco['annotations'] if ann['image_id'] in subset_image_ids]\n",
    "\n",
    "# (optional) keep only categories that are actually used in this subset\n",
    "used_cat_ids = {ann['category_id'] for ann in subset_anns}\n",
    "subset_cats = [c for c in coco['categories'] if c['id'] in used_cat_ids]\n",
    "\n",
    "# Assemble subset COCO\n",
    "subset = {\n",
    "    'images': subset_images,\n",
    "    'annotations': subset_anns,\n",
    "    'categories': subset_cats,\n",
    "    'info' : [],\n",
    "    'licenses' : []\n",
    "}\n",
    "\n",
    "# Write\n",
    "COCO_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(COCO_OUT, 'w') as f:\n",
    "    json.dump(subset, f, indent=2)\n",
    "\n",
    "print(f\"✅ Wrote {COCO_OUT}\")\n",
    "print(f\"   {len(subset_images)} images, {len(subset_anns)} annotations, {len(subset_cats)} categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "547194de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Converted 2300 detections (skipped 0 with unknown filenames).\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# ---- paths\n",
    "REPO_ROOT = Path.cwd().parent  # adjust if needed\n",
    "gt_json   = REPO_ROOT/'data/processed/JSONs/test_annotations_coco.json'\n",
    "pred_json = REPO_ROOT/'runs/segment/Yolo8sTest19/predictions.json'  # <- your run\n",
    "out_json  = REPO_ROOT/'runs/segment/Yolo8sTest19/predictions_coco.json'\n",
    "\n",
    "# ---- load GT & build filename -> image_id map\n",
    "gt = COCO(str(gt_json))                  # also validates schema\n",
    "with open(gt_json) as f:\n",
    "    gt_raw = json.load(f)\n",
    "name_to_id = {im['file_name']: im['id'] for im in gt_raw['images']}\n",
    "gt_cat_ids = {c['id'] for c in gt_raw['categories']}   # e.g., {1,2}\n",
    "\n",
    "# ---- load Ultralytics predictions\n",
    "with open(pred_json) as f:\n",
    "    preds = json.load(f)                 # list of dicts\n",
    "\n",
    "# Detect whether prediction category ids are 0-based (e.g., {0,1}) and map to GT\n",
    "pred_cat_ids = {int(p['category_id']) for p in preds}\n",
    "needs_plus_one = (min(pred_cat_ids) == 0 and min(gt_cat_ids) == 1)\n",
    "\n",
    "results = []\n",
    "skipped_no_image = 0\n",
    "for d in preds:\n",
    "    # Ultralytics writes the filename into image_id (string). Map to numeric id.\n",
    "    fname = d.get('image_id') or d.get('file_name')\n",
    "    fname = fname + '.tif'\n",
    "    if fname not in name_to_id:\n",
    "        skipped_no_image += 1\n",
    "        continue\n",
    "\n",
    "    cat_id = int(d['category_id'])\n",
    "    if needs_plus_one:\n",
    "        cat_id = cat_id + 1\n",
    "\n",
    "    rec = {\n",
    "        'image_id': name_to_id[fname],\n",
    "        'category_id': cat_id,\n",
    "        'score': float(d['score']),\n",
    "    }\n",
    "    # keep whichever you want to evaluate: bbox and/or segm\n",
    "    if 'bbox' in d:\n",
    "        rec['bbox'] = [float(x) for x in d['bbox']]     # [x,y,w,h]\n",
    "    if 'segmentation' in d:\n",
    "        # pycocotools accepts COCO RLE dict: {'size':[H,W], 'counts': <str or bytes>}\n",
    "        rec['segmentation'] = d['segmentation']\n",
    "    results.append(rec)\n",
    "\n",
    "print(f\"Converted {len(results)} detections \"\n",
    "      f\"(skipped {skipped_no_image} with unknown filenames).\")\n",
    "\n",
    "# ---- write COCO-results file\n",
    "out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(out_json, 'w') as f:\n",
    "    json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6811dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== BOUNDS (bbox) at IoU=0.75 ==\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.33s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= small | maxDets=100 ] = 0.093\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=  1 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets= 10 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.093\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= small | maxDets=100 ] = 0.093\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= large | maxDets=100 ] = -1.000\n",
      "\n",
      "== MASKS (segm) at IoU=0.75 ==\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.38s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= small | maxDets=100 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=  1 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets= 10 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= small | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def eval_with_cocoeval(gt_json, yolo_preds_json, iou=0.75, iouType='segm'):\n",
    "    \"\"\"\n",
    "    gt_json: COCO GT file (has images/annotations/categories)\n",
    "    yolo_preds_json: Ultralytics predictions.json (list of det dicts, or a dict\n",
    "                     from which we can pull a list)\n",
    "    iouType: 'segm' for masks, 'bbox' for boxes\n",
    "    \"\"\"\n",
    "    cocoGt = COCO(str(gt_json))\n",
    "\n",
    "    # Build maps so we can turn filename strings into integer image IDs\n",
    "    fname2id = {img[\"file_name\"]: img[\"id\"] for img in cocoGt.dataset[\"images\"]}\n",
    "    stem2id = {Path(k).stem: v for k, v in fname2id.items()}\n",
    "    valid_img_ids = set(fname2id.values())\n",
    "    valid_cat_ids = set(cocoGt.getCatIds())\n",
    "\n",
    "    # Load predictions and normalize to a LIST of result dicts\n",
    "    with open(yolo_preds_json) as f:\n",
    "        data = json.load(f)\n",
    "    dets = data[\"annotations\"] if isinstance(data, dict) and \"annotations\" in data else data\n",
    "    assert isinstance(dets, list), \"Predictions must be a LIST of detection objects\"\n",
    "\n",
    "    norm = []\n",
    "    for d in dets:\n",
    "        # --- image_id: convert filename -> int id if needed\n",
    "        iid = d.get(\"image_id\")\n",
    "        if isinstance(iid, str):\n",
    "            iid = (\n",
    "                fname2id.get(iid)\n",
    "                or fname2id.get(iid + \".tif\")\n",
    "                or fname2id.get(iid + \".png\")\n",
    "                or stem2id.get(iid)\n",
    "            )\n",
    "            if iid is None:\n",
    "                continue  # skip preds not in the GT split\n",
    "        if iid not in valid_img_ids:\n",
    "            continue\n",
    "\n",
    "        # --- category_id: fix 0/1 vs 1/2 if necessary\n",
    "        cid = int(d[\"category_id\"])\n",
    "        if cid not in valid_cat_ids and (cid + 1) in valid_cat_ids:\n",
    "            cid += 1\n",
    "        if cid not in valid_cat_ids:\n",
    "            continue\n",
    "\n",
    "        out = {\n",
    "            \"image_id\": int(iid),\n",
    "            \"category_id\": cid,\n",
    "            \"score\": float(d[\"score\"]),\n",
    "        }\n",
    "        if \"bbox\" in d:\n",
    "            x, y, w, h = d[\"bbox\"]\n",
    "            out[\"bbox\"] = [float(x), float(y), float(w), float(h)]  # [x,y,w,h]\n",
    "        if \"segmentation\" in d:\n",
    "            out[\"segmentation\"] = d[\"segmentation\"]  # RLE/polygons are OK as-is\n",
    "\n",
    "        norm.append(out)\n",
    "\n",
    "    # Feed detections to COCO (must be a LIST)\n",
    "    cocoDt = cocoGt.loadRes(norm)\n",
    "\n",
    "    # Evaluate at IoU = 0.75 only\n",
    "    cocoEvaluation = COCOeval(cocoGt, cocoDt, iouType=iouType)\n",
    "    cocoEvaluation.params.iouThrs = np.array([iou], dtype=np.float64)\n",
    "    cocoEvaluation.evaluate()\n",
    "    cocoEvaluation.accumulate()\n",
    "    cocoEvaluation.summarize()\n",
    "\n",
    "# ---- use it ----\n",
    "gt = REPO_ROOT/ \"data/processed/JSONs/test_annotations_coco.json\"\n",
    "preds = REPO_ROOT/\"runs/segment/Yolo8sTest19/predictions.json\"\n",
    "\n",
    "print(\"== BOUNDS (bbox) at IoU=0.75 ==\")\n",
    "eval_with_cocoeval(gt, preds, iou=0.75, iouType=\"bbox\")\n",
    "\n",
    "print(\"\\n== MASKS (segm) at IoU=0.75 ==\")\n",
    "eval_with_cocoeval(gt, preds, iou=0.75, iouType=\"segm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00e0e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# ---- paths\n",
    "REPO_ROOT = Path.cwd().parent  # adjust if needed\n",
    "groundtruth_json   = REPO_ROOT/'data/processed/JSONs/test_annotations_coco.json'\n",
    "predictions_json = REPO_ROOT/'runs/segment/Yolo8sTest19/predictions_coco.json'  # <- your run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab582405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_groundtruth = COCO(groundtruth_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d76a1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "with open(predictions_json) as f:\n",
    "    detections = json.load(f)\n",
    "coco_detections = coco_groundtruth.loadRes(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23d2bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.45s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=4000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=4000 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= small | maxDets=4000 ] = 0.080\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=medium | maxDets=4000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= large | maxDets=4000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=200 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=4000 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= small | maxDets=4000 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=medium | maxDets=4000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= large | maxDets=4000 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "Evaluation = COCOeval(coco_groundtruth, coco_detections)\n",
    "\n",
    "Evaluation.params.iouThrs = np.array([0.75], dtype=np.float64)   # evaluate only at IoU=0.75\n",
    "Evaluation.params.maxDets = [100, 200, 4000]                     # change max detections per image\n",
    "Evaluation.params.iouType = 'segm'\n",
    "Evaluation.params.useCats = False\n",
    "Evaluation.params.catIds = coco_groundtruth.getCatIds()\n",
    "\n",
    "Evaluation.evaluate()\n",
    "Evaluation.accumulate()\n",
    "Evaluation.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c654d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=4000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=4000 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= small | maxDets=4000 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area=medium | maxDets=4000 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.75:0.75 | area= large | maxDets=4000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=100 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=200 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=   all | maxDets=4000 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= small | maxDets=4000 ] = 0.057\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area=medium | maxDets=4000 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.75:0.75 | area= large | maxDets=4000 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "Evaluation = COCOeval(coco_groundtruth, coco_detections)\n",
    "\n",
    "Evaluation.params.iouThrs = np.array([0.75], dtype=np.float64)   # evaluate only at IoU=0.75\n",
    "Evaluation.params.maxDets = [100, 200, 4000]                     # change max detections per image\n",
    "Evaluation.params.iouType = 'segm'\n",
    "Evaluation.params.useCats = True\n",
    "Evaluation.params.catIds = coco_groundtruth.getCatIds()\n",
    "\n",
    "Evaluation.evaluate()\n",
    "Evaluation.accumulate()\n",
    "Evaluation.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7040455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T (IoUs): [0.75]\n",
      "A (areas): ['all', 'small', 'medium', 'large']\n",
      "M (maxDets): [100, 200, 4000]\n",
      "K (classes): ['individual_tree', 'group_of_trees']\n"
     ]
    }
   ],
   "source": [
    "precision = Evaluation.eval['precision']          # [T,R,K,A,M]\n",
    "t = np.where(np.isclose(Evaluation.params.iouThrs, 0.75))[0][0]  # number of IoU thresholds. I chose IoU=0.75  -> T index\n",
    "k = 0   # number of categories / choose category index (0..K-1)                 -> K index\n",
    "a = 0   # choose area range (0=all)                      -> A index\n",
    "m = -1  # choose the largest maxDets (usually last)      -> M index\n",
    "\n",
    "pr  = precision[t, :, k, a, m]            # precision at each recall sample R\n",
    "rec = Evaluation.params.recThrs           # the recall grid (x-axis)\n",
    "mask = pr > -1                   # COCO fills missing with -1\n",
    "rec, pr = rec[mask], pr[mask]\n",
    "f1 = 2*pr*rec/(pr+rec+1e-12)\n",
    "\n",
    "print('T (IoUs):', Evaluation.params.iouThrs)\n",
    "print('A (areas):', Evaluation.params.areaRngLbl)\n",
    "print('M (maxDets):', Evaluation.params.maxDets)\n",
    "cats = coco_groundtruth.loadCats(Evaluation.params.catIds)\n",
    "print('K (classes):', [c['name'] for c in cats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06e724d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98851, 0.96049, 0.94744, 0.93647, 0.92645, 0.90846, 0.87486,\n",
       "       0.76867, 0.32044, 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     , 0.     , 0.     , 0.     , 0.     ,\n",
       "       0.     , 0.     , 0.     ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = Evaluation.eval['scores'][t, :, k, a, m][mask]  # confidence per PR point\n",
    "\n",
    "scores\n",
    "#That’s all those indices mean:\n",
    "\n",
    "# t → which IoU threshold,\n",
    "\n",
    "# ( : ) on axis R → sweep recall points for the PR curve,\n",
    "\n",
    "# k → which class,\n",
    "\n",
    "# a → which area bin,\n",
    "\n",
    "# m → which maxDet setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af6b20dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels with no image: []\n",
      "Images with no label: ['.DS_Store']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT\n",
    "\n",
    "img_dir = Path(REPO_ROOT/ \"data/processed/images/test\")\n",
    "lab_dir = Path(REPO_ROOT/\"data/processed/labels/test\")\n",
    "\n",
    "img_stems = {p.stem for p in img_dir.glob(\"*.*\")}     # *.tif, *.png, etc.\n",
    "lab_stems = {p.stem for p in lab_dir.glob(\"*.txt\")}\n",
    "\n",
    "print(\"Labels with no image:\", sorted(lab_stems - img_stems))\n",
    "print(\"Images with no label:\", sorted(img_stems - lab_stems))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "92bb0199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem images: [('.DS_Store', 'cv2.imread returned None')]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "bad = []\n",
    "for p in Path(REPO_ROOT/\"data/processed/images/test\").glob(\"*.*\"):\n",
    "    im = cv2.imread(str(p), cv2.IMREAD_UNCHANGED)\n",
    "    if im is None:\n",
    "        bad.append((p.name, \"cv2.imread returned None\"))\n",
    "    else:\n",
    "        h, w = im.shape[:2]\n",
    "        if h == 0 or w == 0:\n",
    "            bad.append((p.name, f\"zero shape {im.shape}\"))\n",
    "\n",
    "print(\"Problem images:\", bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc601f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv3.10 (PyTorch)",
   "language": "python",
   "name": "mlenv3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
